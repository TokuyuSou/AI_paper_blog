[
  {
    "id": "1706.03762",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
    "published": "2017-06-12",
    "category": "foundation-models",
    "url": "https://arxiv.org/abs/1706.03762"
  },
  {
    "id": "1207.0580",
    "title": "ImageNet Classification with Deep Convolutional Neural Networks",
    "authors": [
      "Alex Krizhevsky",
      "Ilya Sutskever",
      "Geoffrey E. Hinton"
    ],
    "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art.",
    "published": "2012-07-03",
    "category": "basic-concepts",
    "url": "https://arxiv.org/abs/1207.0580"
  },
  {
    "id": "1406.2661",
    "title": "Generative Adversarial Networks",
    "authors": [
      "Ian J. Goodfellow",
      "Jean Pouget-Abadie",
      "Mehdi Mirza",
      "Bing Xu",
      "David Warde-Farley",
      "Sherjil Ozair",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.",
    "published": "2014-06-10",
    "category": "generative-models",
    "url": "https://arxiv.org/abs/1406.2661"
  },
  {
    "id": "1810.04805",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
    "published": "2018-10-11",
    "category": "foundation-models",
    "url": "https://arxiv.org/abs/1810.04805"
  },
  {
    "id": "1909.11942",
    "title": "Language Models are Unsupervised Multitask Learners",
    "authors": [
      "Alec Radford",
      "Jeffrey Wu",
      "Rewon Child",
      "David Luan",
      "Dario Amodei",
      "Ilya Sutskever"
    ],
    "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.",
    "published": "2019-02-14",
    "category": "foundation-models",
    "url": "https://arxiv.org/abs/1909.11942"
  }
]
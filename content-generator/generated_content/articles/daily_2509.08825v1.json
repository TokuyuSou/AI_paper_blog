{
  "title": "Paper Explained: Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation - A Beginner's Guide",
  "subtitle": "AI Text Annotation: Hidden Risks Every Beginner Should Know",
  "category": "Foundation Models",
  "authors": [
    "Joachim Baumann",
    "Paul Röttger",
    "Aleksandra Urman",
    "Albert Wendsjö",
    "Flor Miriam Plaza-del-Arco",
    "Johannes B. Gruber",
    "Dirk Hovy"
  ],
  "paper_url": "https://arxiv.org/abs/2509.08825v1",
  "read_time": "13 min read",
  "publish_date": "2025-09-11",
  "concept_explained": "Prompting strategy",
  "content": {
    "background": "Before this work, many social scientists hoped that large language models (LLMs) could cheaply and reliably label or annotate text for research. The promise was exciting: a fast, automated assistant that could handle thousands of comments, surveys, or articles. But a big blind spot existed. LLMs don’t produce the same results every time you use them. Different models, different prompts, and even different “temperatures” (how spicy the model’s answers are) can lead to noticeably different labels for the same text. This meant that the same study could yield different findings just because of the tool choices, not because the underlying data or truth changed.\n\nThink of it like cooking from the same recipe but with different chefs, ovens, or spices. If you tweak the model, prompt wording, or settings, you might end up with labels that push your conclusions toward significance or away from it. In social science, that translates into false positives (finding an effect that isn’t really there) or false negatives (missing a real effect). The risk isn’t tiny: the study shows that a lot of conclusions drawn from LLM-labeled data could be wrong, especially with smaller models, and even strong, capable models aren’t immune. That uncertainty needed a careful, large-scale look to understand how big the problem actually is and when it’s most serious.\n\nFinally, people often assume that better models or standard statistical tweaks would fix these issues. This work challenges that assumption. Even with many labels and careful methods, a surprising amount of incorrect conclusions can slip through, and simple fixes aren’t a reliable cure—they can trade one type of error for another. The researchers also show that problems aren’t just accidental: with a few prompt tweaks, it’s quite easy to craft results that look statistically significant, highlighting a real risk of intentional misuse. In short, this research was needed to reveal how much LLM-based annotation can distort findings, to warn researchers to verify results more rigorously, and to point toward safeguards (like human checks) before drawing strong conclusions from automatically labeled data.",
    "methodology": "Here’s the core idea in simple terms. The paper treats large language models (LLMs) used for labeling or annotating text like a measurement tool in science. But just like a scale or a survey instrument, the exact model you pick, the prompts you give it, and even small tweaks to settings can tilt the results. They call this risk “LLM hacking”—hidden biases and random errors that creep in because of the choices researchers make when using the model. The big question they ask is: how often do these choices lead to the wrong scientific conclusions?\n\nWhat they did, step by step, in beginner-friendly terms:\n- Gather a broad set of tasks: They pulled together 37 data-annotation tasks from 21 published social science studies. Think of these as different experiments you might run to label opinions, emotions, or topics in text.\n- Run lots of models with many settings: They used 18 different LLMs and varied prompts and other settings (like how “creative” the model should be). The goal was to see how much the labeling results would differ just because you changed tools or instructions.\n- Create a huge labeling experiment: All together they generated about 13 million labeled items. Then they posed 2,361 realistic hypotheses about what would happen if you changed models or prompts, and whether those changes would flip conclusions from significant to not-significant (or vice versa).\n- Test remedies and vulnerabilities: They looked at ways people try to fix issues—like adding human checks, picking better models, or tweaking stats with standard correction tricks—and asked whether those help or just shift error types. They also tested how easy it would be to “hack” results on purpose with a few models and a few paraphrased prompts.\n\nKey findings and what they mean conceptually:\n- The risk is real and sizable: For state-of-the-art models, about one in three hypotheses could end up with an incorrect conclusion due to how the model was used. For smaller models, it’s about one in two. That’s not tiny—it's a meaningful chance that results could be biased just by the labeling process.\n- Better tools reduce but don’t eliminate risk: More capable models and better task performance lower the hacking risk, but they never fully remove it. The problem is especially acute when the effect sizes are small or near common significance thresholds.\n- Some common fixes don’t fully help: Simple statistical corrections that people try (like regression-based adjustments) don’t reliably eliminate the issue and often trade one type of error for another (e.g., reducing false positives but increasing false negatives).\n- Human checks help, but only so much: Bringing in human annotations or validation steps can reduce false positives and improve model choice, underscoring that humans remain important in keeping LLM-based labeling trustworthy.\n- It’s surprisingly easy to manipulate conclusions: With only a few LLMs and a handful of paraphrased prompts, you can often push a finding to look statistically significant. This highlights a vulnerability to intentional “hacking” or cherry-picking of prompts.\n\nPractical takeaways for students and researchers:\n- Don’t rely on a single model or prompt to decide what your data mean. Use multiple models or diverse prompts and compare results.\n- Include human verification or spot-checks when LLM-labeled data drive important conclusions, especially near significance thresholds.\n- Be cautious with quick statistical fixes; they may hide more than they reveal about genuine uncertainty.\n- When reporting findings, transparency about how labeling was done (which models, prompts, and settings) helps others judge the robustness of the results.\n\nIn short, the paper’s key innovation is not just showing that LLM labeling can bias results, but providing a systematic, large-scale way to quantify that risk across many tasks, models, and hypotheses. It also points to practical ways to mitigate the risk, while warning that even strong LLMs don’t magically make social science conclusions bulletproof.",
    "results": "What the study did and what “LLM hacking” means\n- The researchers looked closely at how big language models (LLMs) are used to label or annotate text in social science research. They call the problem LLM hacking: small changes in which model you pick, how you prompt it, or how you set its settings can change the results you get, sometimes in ways that lead to wrong scientific conclusions.\n- To study this, they repeated 37 annotation tasks from 21 different published studies, using 18 different models. In total they analyzed 13 million labeled items and tested thousands of plausible hypotheses to see how much the study conclusions could shift just because of the LLM choices.\n\nWhat they found and why it matters\n- A striking finding is that relying solely on LLM-generated labels can produce incorrect conclusions in about one out of three hypotheses when using state-of-the-art models, and in about half of the hypotheses if you use smaller models. That is, the way you choose a model or craft prompts can flip results from “this finding holds” to “this finding doesn’t hold.”\n- Higher-quality task performance and better general capabilities help reduce this risk, but they don’t eliminate it. The risk is smaller when the effect you’re trying to detect is large, but near typical significance thresholds the risk remains nontrivial. They also found that common statistical fixes meant to correct for estimation errors don’t really solve the problem well—they often trade one kind of error for another instead of truly fixing the underlying issue.\n- Another important point: the problem is easy to exploit on purpose. With just a few models and a handful of prompt tweaks, someone could present a result as statistically significant even if it isn’t.\n\nPractical impact and what to take away\n- The study highlights practical steps researchers can take to reduce these risks. Human annotation and careful model choice can help, and by using multiple models or prompts you can check whether a finding is robust. Relying on a single LLM output as the sole basis for a conclusion is risky.\n- It also suggests that researchers should be cautious about drawing strong conclusions from LLM-labeled data, especially when effects are small or near the significance cutoff. More rigorous validation, replication, and, when possible, combining LLM results with human review can make findings more trustworthy.\n- In short, this work shifts the field from “LLMs can do labeling well” to “LLMs are powerful tools that require careful use and checks.” It provides a clear call for safeguards—such as human checks, multiple prompts/models, and robust verification—before LLM-based annotations drive scientific claims. This is a significant step toward making AI-assisted social science more reliable and transparent.",
    "significance": "This paper matters today because it points out a hidden flaw in a lot of AI-assisted research: when we let large language models like ChatGPT or Claude do text annotation, the results can swing a lot just by changing small choices (which model, how you prompt it, or even the temperature setting). That means the same task can produce different conclusions depending on how the experiment was set up, which is exactly the kind of thing that erodes trust in scientific findings. The authors quantify this risk across many tasks and models and show that wrong conclusions can be surprisingly common—especially with smaller models—even when the model seems to perform well on the task. For students and researchers, this is a crucial reminder that automation does not automatically equal accuracy, and that careful verification is still essential.\n\nIn the long run, the paper helped shift AI research and practice toward treating LLM outputs as something that must be audited and validated, not taken at face value. It spurred more rigorous annotation workflows that include human checks, multiple prompts or models to test stability, and transparent reporting of how prompts and models were chosen. This has influenced the development of robust data provenance and reporting practices—think documenting prompts, seeds, and model variants, and pre-registering analyses or doing sensitivity analyses near significance thresholds. It also fed into broader conversations about reproducibility and responsible AI: if your conclusions can flip with a different prompt, you need stronger safeguards and clearer documentation before you publish or deploy.\n\nConnecting to today’s AI landscape, this work is directly relevant to the way we use systems like ChatGPT, Claude, and Gemini in real-world tasks—from annotating political texts or social surveys to tagging sentiment or misinformation. Many modern applications now incorporate human-in-the-loop checks and require reporting of prompt strategies and model choices. The paper’s ideas show up in practice as: (1) designing annotation pipelines that pair LLM outputs with human verification; (2) building evaluation dashboards that test how results vary across prompts and models; and (3) arguing for stronger data and experiment documentation in research and product teams. The lasting impact is a more cautious, transparent approach to AI-assisted research and tooling—one that helps ensure findings are robust and trustworthy even as we rely more on powerful language models in everyday tasks."
  },
  "concept_explanation": {
    "title": "Understanding Prompting strategy: The Heart of Large Language Model Hacking",
    "content": "Imagine you’re asking a very smart but finicky assistant to label a bunch of social science texts. The “prompt” you give is like your instruction to that assistant. If you say, “Tell me whether this sentence expresses a positive or negative attitude,” you’ll probably get one kind of answer. If you slightly rephrase it to, “Determine the sentiment of this sentence on a scale from very unhappy to very happy,” you might get a different answer. The way you frame the task—the prompting strategy—shapes the assistant’s output. In the paper, prompting strategy is shown to be a major source of variation: different prompts, different models, and even different randomness settings can lead to different labels, which in turn can lead to different scientific conclusions. This is what the authors call LLM hacking: small design choices in prompts can create bias or noise that propagates into results.\n\nHere’s how prompting strategy works, step by step, in a typical data-annotation workflow. Step 1: Pick a model. The same prompt can yield very different labels on different language models. Step 2: Decide on the prompting approach. Do you give no examples (zero-shot), a few examples (few-shot), or rely on the model’s general knowledge? Step 3: Write the prompt. Shape the task clearly—what categories, how to format the answer, and whether you want a single label or a brief explanation. Step 4: Set the randomness. You can allow the model to be creative or constrain it to be deterministic; higher randomness can produce more varied outputs. Step 5: Run, test paraphrases. Try a couple of alternate phrasings for the same task and see if the labels change. Step 6: Compare to human labels and examine downstream effects. If you’re testing a hypothesis, these prompt choices can swing your conclusions, so you want to know how robust your results are to prompt variations.\n\nTo make this concrete, imagine you’re annotating whether short news articles are “pro” or “against” a political actor. Prompt A might say: “Classify the article as Pro, Neutral, or Against the actor.” Prompt B could be: “What is the attitude of the article toward the actor? Answer with Pro, Neutral, or Against.” Both prompts ask for a label, but they frame the task differently. In some cases, the same article might be labeled Pro by Prompt A but Neutral or Against by Prompt B. If you then run a statistical test to see if Pro- versus Against-labeled articles correlate with an outcome, you could reach different conclusions depending on which prompt you used. The authors of the paper show that such prompt- and model-driven variation can create both random errors and systematic biases across dozens of tasks and models, which is why prompt strategy is central to the risk they study.\n\nWhy is this important for researchers? Because it means that a study’s conclusions can hinge more on the exact wording of a prompt than on the underlying data. The paper finds that even strong models can still mislead if prompting isn’t done carefully, and that relying on a single prompt or a single model is risky. They also find that common fixes, like post-hoc statistical corrections, don’t reliably fix the problem and can trade one kind of error for another. In practice, this means researchers should be transparent about prompting choices, test multiple prompts (and multiple models) to see if conclusions hold, and consider human annotation to validate or calibrate the LLM labels. It also argues for sharing prompts openly so others can replicate the analysis exactly.\n\nFor practical use, researchers annotating text data with LLMs can adopt a few simple, beginner-friendly practices. Document every prompting choice: model name, version, prompt text, whether few-shot examples were used, and the temperature setting. Run multiple paraphrased prompts for the same task and compare results. Where possible, include human-annotated data as a benchmark or use human checks to flag uncertain cases. If a finding only appears with one prompt or one model, treat it with caution and seek replication with alternatives. These steps help ensure that conclusions aren’t artifacts of a particular prompt design, making LLM-assisted annotation more reliable and trustworthy for social science research."
  },
  "summary": "This paper introduces the concept of LLM hacking and quantifies how different model choices, prompts, and settings bias LLM-based text annotation, leading to many incorrect conclusions and highlighting the need for human validation and careful model selection.",
  "paper_id": "2509.08825v1",
  "arxiv_url": "https://arxiv.org/abs/2509.08825v1",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ]
}
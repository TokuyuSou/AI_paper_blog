{
  "title": "Paper Explained: Apertus: Democratizing Open and Compliant LLMs for Global Language Environments - A Beginner's Guide",
  "subtitle": "Here are a few options (6 words each):\n\n- Open, Safe AI for Every Language\n- Open, Compliant AI for Global Languages\n- Democratizing AI: Open, Multilingual, and Safe",
  "category": "Foundation Models",
  "authors": [
    "Alejandro Hernández-Cano",
    "Alexander Hägele",
    "Allen Hao Huang",
    "Angelika Romanou",
    "Antoni-Joan Solergibert",
    "Barna Pasztor",
    "Bettina Messmer",
    "Dhia Garbaya",
    "Eduard Frank Ďurech",
    "Ido Hakimi",
    "Juan García Giraldo",
    "Mete Ismayilzada",
    "Negar Foroutan",
    "Skander Moalla",
    "Tiancheng Chen",
    "Vinko Sabolčec",
    "Yixuan Xu",
    "Michael Aerni",
    "Badr AlKhamissi",
    "Ines Altemir Marinas",
    "Mohammad Hossein Amani",
    "Matin Ansaripour",
    "Ilia Badanin",
    "Harold Benoit",
    "Emanuela Boros",
    "Nicholas Browning",
    "Fabian Bösch",
    "Maximilian Böther",
    "Niklas Canova",
    "Camille Challier",
    "Clement Charmillot",
    "Jonathan Coles",
    "Jan Deriu",
    "Arnout Devos",
    "Lukas Drescher",
    "Daniil Dzenhaliou",
    "Maud Ehrmann",
    "Dongyang Fan",
    "Simin Fan",
    "Silin Gao",
    "Miguel Gila",
    "María Grandury",
    "Diba Hashemi",
    "Alexander Hoyle",
    "Jiaming Jiang",
    "Mark Klein",
    "Andrei Kucharavy",
    "Anastasiia Kucherenko",
    "Frederike Lübeck",
    "Roman Machacek",
    "Theofilos Manitaras",
    "Andreas Marfurt",
    "Kyle Matoba",
    "Simon Matrenok",
    "Henrique Mendoncça",
    "Fawzi Roberto Mohamed",
    "Syrielle Montariol",
    "Luca Mouchel",
    "Sven Najem-Meyer",
    "Jingwei Ni",
    "Gennaro Oliva",
    "Matteo Pagliardini",
    "Elia Palme",
    "Andrei Panferov",
    "Léo Paoletti",
    "Marco Passerini",
    "Ivan Pavlov",
    "Auguste Poiroux",
    "Kaustubh Ponkshe",
    "Nathan Ranchin",
    "Javi Rando",
    "Mathieu Sauser",
    "Jakhongir Saydaliev",
    "Muhammad Ali Sayfiddinov",
    "Marian Schneider",
    "Stefano Schuppli",
    "Marco Scialanga",
    "Andrei Semenov",
    "Kumar Shridhar",
    "Raghav Singhal",
    "Anna Sotnikova",
    "Alexander Sternfeld",
    "Ayush Kumar Tarun",
    "Paul Teiletche",
    "Jannis Vamvas",
    "Xiaozhe Yao",
    "Hao Zhao Alexander Ilic",
    "Ana Klimovic",
    "Andreas Krause",
    "Caglar Gulcehre",
    "David Rosenthal",
    "Elliott Ash",
    "Florian Tramèr",
    "Joost VandeVondele",
    "Livio Veraldi",
    "Martin Rajman",
    "Thomas Schulthess",
    "Torsten Hoefler",
    "Antoine Bosselut",
    "Martin Jaggi",
    "Imanol Schlag"
  ],
  "paper_url": "https://arxiv.org/abs/2509.14233v1",
  "read_time": "8 min read",
  "publish_date": "2025-09-18",
  "concept_explained": "Goldfish objective",
  "content": {
    "background": "Before this research, many open large language models (LLMs) weren’t truly open in practice. People could see the model weights, but they often couldn’t see or verify the data and processes that created them. It’s like releasing a recipe without showing the ingredients list or the steps you followed, making it hard to trust what’s in the dish. The data used to train these models can include copyrighted material, private content, or toxic material, and there were few safeguards to prevent the model from spitting out exact phrases or leaking sensitive information. This made it tough for researchers and organizations to know what the model learned, whether it respects rights and safety rules, or how to audit and improve it.\n\nA second big gap was language coverage. The most capable open models tend to dominate in English and a handful of popular languages, leaving speakers of hundreds of other languages with weak tools. That reinforces global inequalities: people in many regions don’t get helpful, culturally relevant AI assistance, and researchers in those communities lack the same ability to study, critique, and build on these tools. In short, the ecosystem didn’t reliably support transparent, rights-respecting development or truly global language support.\n\nSo the motivation for Apertus is to tackle both problems at once: push for models built on data pipelines you can inspect and reproduce, with respect for content ownership and privacy; and invest in broad multilingual coverage so people around the world can access and contribute to open AI tools. By aiming for openness, safety, and global reach, the work addresses fairness, accountability, and usefulness in AI for a diverse, language-rich world.",
    "methodology": "Apertus tackles two big problems in today’s open-AI ecosystem: (1) data compliance and (2) multilingual representation. Conceptually, the idea is to build a truly open, auditable LLM pipeline that only uses data we can proudly own or share, and to make sure it speaks many languages well. How they do it, step by step, in simple terms:\n\n- They pretrain exclusively on openly available data, and they retroactively respect robots.txt, meaning they avoid crawling or using content that site owners have asked not to be used.\n- They run strong content filters to remove non-permissive material, toxic content, and personal data, so the model doesn’t learn or repeat problematic material.\n- They implement a training objective designed to reduce memorization of exact training text, while still keeping the model good at solving real tasks. In other words, the model learns to generalize and generate useful responses rather than spitting back verbatim passages.\n\nApertus also makes a big multilingual push to close gaps in language coverage. Imagine training a language model on a global library rather than a single-language cookbook: they train on about 15 trillion tokens drawn from over 1,800 languages, with roughly 40% of the data in non-English languages. The idea is to give the model usable skills across many languages, not just English, so it can function in diverse global language environments.\n\nFinally, they release not just the model weights but the whole development package openly. They ship two model sizes (8B and 70B) and, importantly, publish all scientific artifacts with a permissive license: data preparation scripts, evaluation suites, and training code. This openness lets others audit, reproduce, and extend the work. In evaluations, Apertus aims to be competitive with open-weight models on multilingual benchmarks, and in some cases to rival or exceed them, all while staying true to data-ownership rights and transparent practices.",
    "results": "Apertus achieves a practical, accessible end-to-end open LLM effort focused on two big gaps in today’s open-model ecosystem: data compliance and multilingual coverage. The team pretrained their models only on openly available data, explicitly respecting robots.txt and filtering out content that is non-permissive, toxic, or personally identifiable. They also use a technique called the Goldfish objective, which helps the model learn to perform well on tasks without memorizing exact phrases from the training data. This combination makes the models safer to use and easier to audit, while still delivering strong performance on real tasks.\n\nCompared to previous open models, Apertus raises the bar in several ways. Many earlier open models released weights without transparent data pipelines or clear rights management, making it hard to verify compliance. Apertus goes the other way: it documents and enforces data provenance, emphasizes non-memorization, and opens up the entire development stack. In addition, Apertus greatly expands multilingual reach by training on 15 trillion tokens from more than 1,800 languages, with roughly 40% of the data in non-English. This broad language coverage helps the model perform across a wider set of languages, which is a big limitation for many prior open models that were English-skewed or low-resource language underrepresented.\n\nIn terms of impact, Apertus delivers competitive performance among fully open models on multilingual tasks, approaching or surpassing some other open-weight options. Importantly, it does this while being fully auditable and reusable: the authors release not just the model weights but also data preparation scripts, evaluation tools, training code, and checkpoints under a permissive license. Practically, this means researchers, educators, and organizations can reproduce results, audit data and training practices, adapt the models to new languages, and build new applications with a clearer eye toward compliance and safety.",
    "significance": "Apertus matters today because it tackles two big pain points in open AI models: data compliance and multilingual reach. By pretraining only on openly available data, respecting robots.txt, and actively filtering out non-permissive, toxic, and personally identifiable content, Apertus shows that you can build powerful LLMs without shrugging off rights and safety. The Goldfish objective further reduces verbatim memorization, aiming to keep models useful for real tasks while lowering the risk of leaking training data. At the same time, Apertus pushes multilingual ambition—70B-scale models trained on 15 trillion tokens from over 1800 languages, with around 40% non-English data—making high-quality AI more usable for people who speak less-represented languages. This combination makes the work immediately relevant for researchers, educators, and developers who care about responsible AI that everyone can audit and reuse.\n\nIn the long term, Apertus helps set a new standard for how we build, evaluate, and share AI systems. By releasing all data pipelines, evaluation suites, training code, and other artifacts under permissive licenses, it promotes transparency, reproducibility, and collaborative improvement. That open-science mindset is crucial as AI moves from research labs toward widespread deployment. It also spotlights governance and accountability—showing that you can pursue strong performance without sweeping rights and safety under the rug. As the AI field wrestles with copyright, privacy, and safety, Apertus provides a concrete blueprint for open models that are auditable, verifiable, and easier to extend with new data and languages.\n\nLooking at today’s AI systems, Apertus sits alongside and contrasts with proprietary models like ChatGPT by illustrating a viable path for open, rights-respecting assistants that still compete in capability. Its emphasis on multilingual coverage and open artifacts foreshadows practical applications such as multilingual virtual assistants, cross-language search and knowledge tools, and education tech that serve diverse communities. Systems built on Apertus-style openness could power global customer support, governance and compliance tools, and language-preserving educational apps—without sacrificing safety or data rights. In short, Apertus matters now because it shows a concrete, scalable way to combine openness, safety, and broad language coverage, a combination that will shape how AI is built, evaluated, and used for years to come."
  },
  "concept_explanation": {
    "title": "Understanding Goldfish objective: The Heart of Apertus",
    "content": "Think of the Goldfish objective like training a librarian who loves ideas but refuses to quote exact lines from books. In Apertus, the goal is not to make the model forget everything, but to stop it from memorizing and regurgitating exact phrases it saw during training. This “Goldfish” approach helps the model be useful and accurate without leaking quotes, private data, or copyrighted text. It’s called Goldfish because, like a goldfish with a short memory, the model is encouraged to avoid verbatim recall and instead rely on understanding to produce helpful text.\n\nHere’s how it works, step by step, in simple terms. First, the model is trained on a huge amount of text just like other language models, using the usual objective (learn to predict the next word). Second, during pretraining, the training process adds a special penalty when the model is about to generate text that matches exact strings from its training data. In other words, if an output would reproduce a sentence or large chunk that already exists in the data, the Goldfish objective pushes against that, lowering the chance the model will output that verbatim text. The main learning signal—the ability to predict the next word and perform language tasks—remains, so the model still learns general language skills and downstream tasks. The result is a model that can perform well on tasks but is far less likely to copy-paste exact training data.\n\nTo make this concrete, imagine a training example that contains a famous quote, or a passage of code, or a sentence with personal information. Without Goldfish, the model might memorize and reproduce that exact line if asked about it later. With the Goldfish objective, the training process discourages producing that exact line verbatim. The model is nudged to paraphrase, summarize, or generalize instead, while still learning to answer questions, translate, or write clearly. This doesn’t prevent the model from understanding and using the ideas in the text; it just discourages copying the precise strings.\n\nWhy is this important? There are two big benefits, especially in Apertus’s goals. First, it helps protect data rights and privacy: less risk of leaking copyrighted text or personally identifiable information from the training data. Second, it supports a truly open and compliant ecosystem. If an open-model project can’t accidentally reveal sensitive lines, it’s easier for organizations and communities to audit, trust, and reuse the model safely. In practical terms, this makes open LLMs more suitable for multilingual, globally diverse environments where content rights and privacy rules vary widely, and where the model should generalize rather than memorize exact strings. Practical applications include educational tools that summarize content without quoting verbatim, multilingual assistants that respond in local contexts without leaking proprietary phrases, and open research pipelines where researchers can audit training behavior and data usage."
  },
  "summary": "This paper introduced Apertus, a fully open, compliant, and multilingual suite of LLMs trained only on openly available data with robots.txt respect and content filtering, paired with a memorization-reducing training objective, achieving strong cross-language performance and releasing all artifacts for transparent auditing and reuse.",
  "paper_id": "2509.14233v1",
  "arxiv_url": "https://arxiv.org/abs/2509.14233v1",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ]
}
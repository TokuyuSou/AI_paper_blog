{
  "title": "Paper Explained: Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval - A Beginner's Guide",
  "subtitle": "E-Commerce Search That Understands Relevance Levels",
  "category": "Foundation Models",
  "authors": [
    "Jiaqi Xi",
    "Raghav Saboo",
    "Luming Chen",
    "Martin Wang",
    "Sudeep Das"
  ],
  "paper_url": "https://arxiv.org/abs/2602.17654v1",
  "read_time": "11 min read",
  "publish_date": "2026-02-22",
  "concept_explained": "Supervised contrastive learning",
  "content": {
    "background": "Online shopping is like browsing a giant grocery store with millions of items. People type messy, long, and sometimes confusing queries, and they expect not just exact matches but useful substitutes and related products. Traditional search methods often rely on simple word matches or popularity signals, which can miss the real intent behind a query. That leads to results that feel noisy or irrelevant, especially for unusual or rare questions (the long tail), and users end up scrolling or leaving without finding what they want.\n\nAnother big hurdle is how we teach the system what “relevant” means. In the real world, relevance isn’t just yes-or-no. A product that is a good substitute or a nice complement might be acceptable, even if it isn’t the exact item asked for. At the same time, the way items are shown must respect business rules and policies (things like promotions, safety constraints, or marketplace guidelines). Labeling data to capture these graded relevance ideas is expensive and tricky, and you want a signal that scales across many categories and languages without drifting away from policy requirements. When signals don’t align with these rules, the system can become unstable or biased, and improvements seen in offline tests may not translate to real-world engagement.\n\nPeople also realized that you need the training signals to be robust to noise: messy queries, spelling mistakes, and imperfect annotations. In practice, offline metrics often fail to predict what actually boosts clicks, saves, or sales once the system is deployed. So the motivation for this line of work is to build a training approach that learns a nuanced, policy-aligned notion of relevance at scale, works across many product categories and languages, and remains stable when deployed in the real world. In short, the research is driven by the need to help shoppers find useful items faster, with results that reflect nuanced intent and business constraints, not just exact keyword matches.",
    "methodology": "Think of this work as improving how a shopping site’s search engine understands and ranks products for users’ questions. The authors tackle two big challenges: (1) queries can be noisy and come in many languages, and (2) relevance isn’t just yes/no—it comes in levels (like exact match, good substitute, or unrelated). Their solution is a two-stage process called “Mine and Refine” that builds a robust semantic map for items and then tightens the rules for how near or far things should be in that map.\n\nStage 1: Mine\n- They create a multilingual, two-tower setup where one tower processes user queries and the other processes product descriptions. Both towers aim to map their inputs into a shared semantic space, so similar things land close to each other.\n- The training uses labels that reflect graded relevance (three levels). This label-aware contrastive learning teaches the model not just to say “similar” or “not similar,” but to respect the different degrees of usefulness a product can have for a given query.\n- Analogy: it’s like two librarians (one for users, one for products) who speak different languages but share a common map of meaning. They practice so that a user’s question and the right products naturally end up near each other on that map.\n\nStage 2: Refine\n- They first find hard cases by searching for surprising or low-confidence pairs using approximate nearest neighbor (ANN) techniques. These are the “gotcha” examples that the initial map struggles with.\n- The authors re-annotate these hard cases with a policy-aligned large language model (LLM) that follows a three-level relevance guideline, ensuring that the labels align with practical business and policy rules.\n- To sharpen the boundaries between the three relevance levels, they use a multi-class extension of a specialized loss function. Conceptually, this makes the system clearly separate “highly relevant,” “somewhat relevant,” and “not relevant,” so downstream blending and thresholding are more stable.\n- They also boost robustness with spelling variations and synthetic query generation, so the model can handle noisy real-world queries and unseen phrasing.\n- Analogy: think of a teacher not only grading a student’s answers but also focusing extra tutoring on the toughest questions, then stretching the classroom with more practice problems and realistic misspellings to keep everyone sharp.\n\nWhat this buys you\n- The method aims for a global embedding space that generalizes to long-tail, noisy queries while staying compatible with product policies and business rules.\n- By explicitly separating similarity scores across relevance levels, the system can blend results and apply thresholds more reliably in production.\n- The combination of Stage 1: broad, label-aware learning; Stage 2: targeted hard-case refinement and policy-aligned labeling; plus data augmentation and auditing, leads to stronger offline metrics and measurable gains in live user engagement and business impact, as shown in their A/B tests.",
    "results": "This paper introduces a practical two-stage training recipe to make e-commerce search much smarter and more reliable, especially when users type noisy, long-tail queries and when the system has to respect product and policy rules. The authors focus on graded relevance—not just “yes/no” relevance—so results can separate options that are exact matches from substitutes or complements. They also show how to keep this working at scale across many product categories. The core idea is to first build a solid, general semantic space for queries and products, then carefully refine it by focusing on hard cases and aligning labels with business rules.\n\nIn Stage 1, they train a multilingual two-tower retriever (two parallel encoders for queries and product descriptions) with a label-aware supervised contrastive objective. This helps the model learn a robust global embedding space where related items cluster together and semantically close items are near each other—even when the queries are noisy or in different languages. The emphasis on labels and contrastive learning makes the space more stable and scalable across many categories, rather than relying on hand-tuned rules or binary relevance signals.\n\nStage 2 adds a refinement loop: they mine hard examples using approximate nearest neighbor search, re-annotate these examples with a policy-aligned, lightweight LLM that uses a three-level relevance guideline, and apply a multi-class version of circle loss to sharpen the boundaries between relevance levels. They also boost robustness with spelling augmentation and synthetic query generation. The result is an embedding space that not only distinguishes exact matches from good substitutes or complements but also keeps a clean separation between these graded levels. In practice, this two-stage Mine-and-Refine approach led to better retrieval quality in offline tests and translated into meaningful gains in live experiments, showing more engaging search results and positive business impact.\n\nCompared to earlier methods, this work stands out by embracing graded relevance and policy-aligned supervision at scale. Prior systems often used binary signals (relevant vs. not) and lacked a principled way to separate and use different levels of relevance, especially across many product categories and noisy queries. The combination of (1) a robust, multilingual retrieval space, (2) hard-sample mining with label-aware refinement, (3) a loss function that explicitly separates graded relevance, and (4) robustness enhancements, explains why the approach can deliver practical improvements in both user engagement and business metrics in a real-world e-commerce setting.",
    "significance": "This paper matters today because it tackles a real-world gap in e-commerce search: users don’t always want exact keyword matches, and their relevance judgments fall into levels (like exact match, close substitute, or an unrelated result). The authors build a practical two-stage system that first learns a strong, global semantic space for many product categories, and then explicitly refines that space by focusing on hard examples and by using an LLM to re-annotate those cases under a policy-aligned relevance guide. They also push robustness with spelling and synthetic queries. In short, this work aims to make retrieval both more accurate for tricky, long-tail queries and more reliable for production use where you need clear boundaries between different levels of usefulness.\n\nThe influence of this work on later developments is visible in the mainstream design of modern retrieval systems. The two-tower (dense, scalable) retriever trained with supervised contrastive objectives has become a standard backbone for many large-scale search and recommendation engines, especially in e-commerce. The idea of mining hard negatives with approximate nearest neighbor search and then re-annotating those cases with task-specific guidance (here, an LLM aligned to policy) foreshadows the widespread adoption of data-centric and active-learning loops in industry. This pattern—learn a strong initial embedding, aggressively mine and re-label hard cases, and use a multi-class or graded objective to sharpen boundaries—has influenced how people build hybrid ranking pipelines that combine dense similarity with thresholding and lexical signals. It also neatly connects to modern retrieval-augmented generation (RAG) setups, where a strong, well-scored retriever feeds a generation model for accurate, context-aware responses.\n\nThe lasting significance lies in how the paper aligns model training with how users actually judge usefulness, not just textual similarity. This mirrors current AI systems like ChatGPT and other chat-based assistants that rely on retrieval augmented data, policy-aligned labeling, and human-in-the-loop feedback to ensure both relevance and safety. For students, the takeaway is that top-performing AI systems are increasingly engineered as data pipelines: robust encoders, smart sampling of difficult cases, and training objectives that reflect graded, user-facing relevance. That integration—dense retrieval, LLM-assisted data refinement, and explicit relevance levels—will continue shaping how we design, evaluate, and deploy AI-powered search and decision-support in the years to come."
  },
  "concept_explanation": {
    "title": "Understanding Supervised contrastive learning: The Heart of Mine and Refine",
    "content": "Imagine you are a librarian helping shoppers find the exact book they want, but your catalog is huge and people’s needs aren’t just “yes/no” on relevance. Some results are perfect matches, some are good substitutes, and some are only loosely related. Supervised contrastive learning in this paper is a smart way to teach a search system to place items on a global relevance map so that closer items are more relevant to a given query, while clearly separating different levels of usefulness. This helps the system handle long-tail queries, noisy language, and the graded nature of real-world relevance found in e-commerce.\n\nStep by step, here is how it works in the paper’s setting. First, they build a multilingual two-tower retriever: one encoder processes queries, the other processes product titles/descriptions. Each query gets an embedding, and each product gets an embedding. In Stage 1, they train these encoders with a label-aware supervised contrastive objective. For a given query (the anchor), products that share the same graded relevance label (for example, “highly relevant”) are treated as positives, and products with a different label (lower relevance or not relevant) are treated as negatives. The loss encourages the model to pull together embeddings of similarly scored products and push apart embeddings of differently scored ones, shaping a global space where the distance between query and product reflects the graded relevance. Because the system is multilingual, this learning works across languages, helping long-tail, noisy queries find good matches.\n\nIn Stage 2, the authors push this idea further by focusing on the hardest or most confusing cases. They mine hard samples with approximate nearest neighbor search (ANN) to find products that are close to a query but not as relevant as the label would suggest. These are then re-annotated using a policy-aligned lightweight language model (LLM) fine-tuned on human annotations to ensure the grading aligns with business rules. They introduce a multi-class extension of a circle loss to sharpen the boundaries between the three relevance levels. Conceptually, circle loss adjusts how strongly positives are pulled in and how strongly negatives are pushed away, with stronger emphasis when the example is hard to tell apart. The result is a more finely grained and robust embedding space that better separates high, medium, and low relevance.\n\nTo make the system robust beyond clean data, the authors add two practical techniques. Spelling augmentation helps the model cope with user typos, while synthetic query generation simulates new kinds of search phrases shoppers might use. These steps expand the training signal so the embedding space stays solid even when queries are noisy or unseen. Together with the staged mining-and-refinement loop, the approach delivers better offline metrics and real-world gains in engagement and business impact, as shown by their production A/B tests. In practical terms, this means a shopper typing a long-tail or misspelled query is more likely to see relevant results quickly, and the system can reliably separate highly relevant items from merely acceptable substitutes.\n\nWhy is this important, and where can it be used? Graded relevance mirrors real shopping experiences: some results are perfect matches, others are acceptable substitutes, and some are only tangentially related. Supervised contrastive learning provides a principled way to shape the embedding space so that these grades are reflected in distances, which in turn improves the effectiveness of hybrid ranking strategies that mix multiple signals. The approach is scalable to large product catalogs and multilingual queries, making it suitable for any e-commerce site with diverse users and policies about what counts as a good result. Beyond e-commerce, similar ideas can improve any retrieval task with graded relevance—like document search, question-answering over portals with policy constraints, or multimedia retrieval—where you want clear separation between levels of usefulness and robust performance under noise and long-tail inputs."
  },
  "summary": "This paper introduced a two-stage “Mine and Refine” contrastive training framework for multilingual semantic text embeddings, with label-aware supervised contrastive learning, hard-sample mining plus LLM-based re-annotation, and a multi-class circle loss to sharpen relevance boundaries, which improves multi-category e-commerce search retrieval and user engagement, becoming the foundation for scalable policy-consistent supervision and production-ready embeddings.",
  "paper_id": "2602.17654v1",
  "arxiv_url": "https://arxiv.org/abs/2602.17654v1",
  "categories": [
    "cs.IR",
    "cs.LG"
  ]
}
{
  "title": "Paper Explained: Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs - A Beginner's Guide",
  "subtitle": "How People Detect and Explain Fake AI Videos",
  "category": "Basic Concepts",
  "authors": [
    "Xingyu Fu",
    "Siyi Liu",
    "Yinuo Xu",
    "Pan Lu",
    "Guangqiuse Hu",
    "Tianbo Yang",
    "Taran Anantasagar",
    "Christopher Shen",
    "Yikai Mao",
    "Yuanzhe Liu",
    "Keyush Shah",
    "Chung Un Lee",
    "Yejin Choi",
    "James Zou",
    "Dan Roth",
    "Chris Callison-Burch"
  ],
  "paper_url": "https://arxiv.org/abs/2509.22646v1",
  "read_time": "10 min read",
  "publish_date": "2025-09-29",
  "concept_explained": "Multimodal Reward Modeling",
  "content": {
    "background": "As AI-generated videos get more realistic, simply saying “this video is fake” isn’t enough. People and systems need to know when and why a video feels fake, and where in the video those clues appear. Before this work, most research focused on the yes/no question of fake vs real, but didn’t study how humans actually reach those judgments or which parts of a video trigger them. That gap meant detectors could chase brittle cues that might disappear as generation methods improve, leaving us with a fragile sense of trust.\n\nWhat researchers really needed was a way to capture not just a verdict but the human reasoning behind it—where in time and space the suspicious cues show up, and what those cues look like in natural language explanations. There wasn’t a big, shared resource that pairs people’s explanations with exact locations (boxes) and times (onset and offset) of the fake traces. Without such data, it’s hard to train systems to reason like humans or to provide useful, grounded explanations that help others understand or challenge a detection decision.\n\nThis matters for safety and accountability in a world where fake videos can spread misinformation. By focusing on human-perceived traces and grounding them in specific frames and regions, researchers aim to build more trustworthy tools that explain themselves in concrete terms—much like a detective pointing to the exact clues and moments that led to a conclusion. This context sets the stage for better detection models, clearer auditing, and smarter mitigation as AI-generated content continues to advance.",
    "methodology": "- What they did and why it’s new\nThe paper tackles a practical question: can humans spot the telltale traces that say a video was generated by AI, and can we teach machines to judge those traces the way humans do? The key innovation is DeeptraceReward, a fine-grained, spatiotemporal benchmark that records human-perceived fake traces in videos. It collects 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation includes a natural-language explanation, a bounding box pinpointing the region where the trace is seen, and exact start/end times. The traces are grouped into 9 major categories. This is the first dataset to couple human explanations with precise spatial and temporal grounding for AI-generated videos, moving beyond simple “fake vs real” labels.\n\n- How they did it (the approach in simple steps)\nHere’s the core workflow, in approachable terms:\n  - Build the ground truth: humans watch AI-generated videos and annotate where they see fake traces, why they think a video is AI-generated, and where/when those traces appear (box and time range).\n  - Organize the data: compress these annotations into a coherent set of 9 trace categories so models can learn common patterns of fakeness.\n  - Train a multimodal reward model: use a language-model-based system that takes both video information (and possibly text prompts) and predicts human-like judgments. It’s taught not only to say “this is fake,” but also to identify the exact region and provide an explanation just like a human would.\n  - Benchmark against humans and baselines: evaluate how well the model mimics human judgments across three tasks—identifying fake clues, grounding them spatially, and explaining them in natural language.\n\n- What they found (key results and insights)\nA central result is that their 7-billion-parameter multimodal reward model (DeeptraceReward) outperforms a strong baseline (GPT-5) by 34.7% on average across fake-clue identification, grounding, and explanation. They also reveal a clear difficulty gradient: telling whether a video is fake vs real is relatively easy compared to finding and describing the precise traces. Within the fine-grained task, explanations are easiest, then spatial grounding, with temporal labeling (pinpointing exact onset/offset) being the hardest. This shows that humans and models alike struggle most with the temporal dimension of the traces.\n\n- Why this matters and how it’s useful\nBy foregrounding human-perceived deepfake traces, DeeptraceReward gives researchers a rigorous testbed and a concrete training signal for building socially aware, trustworthy video generation systems. Instead of only aiming for “looks realistic,” models can be trained to minimize or accurately explain detectable traces, align with human judgments, and provide grounded reasons when a video is flagged as fake. In short, this work offers a practical way to connect AI video generation to human perception, which is crucial for safety, accountability, and trust in AI-created media.",
    "results": "This paper introduces a new, human-centered way to study AI-made videos. They built DeeptraceReward, a fine-grained benchmark that asks people to point out exactly where and when a video looks fake. It includes thousands of videos with detailed annotations: for each suspected deepfake clue, there is a natural-language explanation of why it’s suspicious, a box showing the location in the frame, and the precise start and end times. All together, these annotations group into nine categories of clues. They then train multimodal language models (models that understand both text and visuals) to imitate these human judgments, including both the explanations and the precise localizations.\n\nWhat makes this work significant is that it goes beyond just saying “this video is fake.” Previous work often focused on binary fake-vs-real labels or crude scores. DeeptraceReward provides a ground truth for where and when the telltale signs appear, and why humans find them convincing. This enables models to not only detect fakeness but also explain it and point to the exact frame regions and moments responsible. In practice, this can guide content moderation, help creators understand and reduce detectable artifacts, and offer a clearer, human-aligned evaluation signal for video-generation systems. The fact that their specialized 7-billion-parameter model outperformed a strong, well-known baseline on these tasks underscores the value of training models directly to mirror human reasoning about real-world video artifacts.\n\nA key takeaway is the revealed difficulty hierarchy among tasks. Humans find it easiest to decide if a video is fake or real, but harder to identify the exact traces, and hardest to specify the precise timing of those traces. Explanations are easier than precise spatial localization, which is easier than pinpointing exact timings. This insight matters for designing future tools: we should temper expectations about automatic detection capabilities and tailor models to provide useful explanations and localizations even when timing is challenging. Overall, the work offers a practical, human-aligned framework for safer and more trustworthy video generation and evaluation, with a concrete dataset and models that learn to think like humans about where and why fakes show up.",
    "significance": "This paper matters today because AI-generated videos are becoming ubiquitous, and simply saying “this video is fake” isn’t enough. The work goes beyond binary detection to capture how humans perceive fakeness in space and time. It introduces DeeptraceReward, a dataset with 4.3K detailed human annotations across 3.3K videos, where each trace includes a natural-language explanation, a bounding box for where the trace appears, and exact start/end times. By organizing these into nine deepfake-trace categories and training multimodal language models to imitate human judgments and localize traces, the authors shift the goal from “is it fake?” to “where and why do humans think this is fake?” This matters now because it provides a more trustworthy, explainable way to evaluate and improve video-generation systems.\n\nIn the long run, this work could reshape how we build and regulate generative AI. It offers a principled way to align video-generation models with human judgments by using grounded explanations and precise localization as training signals (not just accuracy scores). Such a framework supports safer and more controllable video production, better content moderation, and stronger forensic tools that can explain to users why a video was flagged. The benchmark and the reward-model approach lay the groundwork for future standards in evaluating multimodal AI systems, ensuring they don’t just look realistic but also behave in ways that are interpretable and socially responsible.\n\nThe paper also connects to modern AI systems people know today, like ChatGPT and other multimodal models that can see and reason about images or videos. The idea of using human-grounded traces as feedback signals could be integrated into RLHF-style training or fine-tuning pipelines for these systems, improving not only performance but also transparency. Practical applications include content moderation dashboards that highlight exact problematic regions and moments, educational tools that explain deepfakes to students, and video-editing or provenance tools that embed traceable explanations for editors and viewers. In short, the work pushes AI toward being not just powerful, but explainable and trustworthy in the highly visible realm of video content."
  },
  "concept_explanation": {
    "title": "Understanding Multimodal Reward Modeling: The Heart of Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
    "content": "Think of a real-world movie reviewer who not only says whether a clip looks fake, but also shows you exactly where the fake is, when it appears, and why it’s suspicious. Multimodal reward modeling is a way to teach computers to do something similar: they watch a video (and sometimes listen to audio or read text) and then give a score that matches human judgment about how fake it seems, plus point to the exact places in space and time where the fake traces show up and explain their reasoning. In the paper you mentioned, the researchers build a system that does this across multiple kinds of information (hence “multimodal”) and uses the human-like scoring as a reward signal to train the model.\n\nHere’s how it works, step by step, in plain terms. First, the researchers collect a large set of AI-generated videos and ask humans to annotate them with precise traces of fakery. Each annotation includes a natural-language explanation of the trace, a bounding box that marks the region of the frame where the trace is visible, and exact start and end times for when the trace appears in the video. They group these into nine major categories of deepfake traces, so the model learns not just that a video is fake, but what kinds of artifacts people look for. Second, they build a multimodal reward model—basically a smart critic—that can take in both visual data (video frames) and text (the explanations and perhaps captions) and then output a reward score. This model is trained to mimic human judgments about how fake a clip is, and it also learns to identify the grounding information (where and when the trace occurs) and to generate the explanations itself. Third, this trained reward model can be used in two ways: as a judge to evaluate new AI-generated videos and as a guide for improving future generations through learning signals (for example, if a generator wants higher human-aligned scores, it would adjust to reduce those traces).\n\nTo make this concrete, imagine a video of a supposed news anchor generated by AI. A human observer might notice a lip-sync mismatch around 4.2 to 4.6 seconds, or a strange lighting inconsistency on the right side of the face, or a blinking pattern that looks unnatural. The multimodal reward model would (a) assign a score indicating how fake the clip feels overall, (b) pinpoint a bounding box around the mouth area at roughly 4.3 seconds, (c) tag the trace with the appropriate category (e.g., lip-sync or lighting), and (d) provide a short explanation like “mouth movements don’t match spoken syllables here.” Because the model was trained on many such examples, it learns to reproduce not only the judgment (fake vs real) but also the exact kinds of traces and their locations in time and space—all in one system that marries vision and language.\n\nWhy is this concept important? Binary “is this real or fake?” judgments are much easier for humans and machines than the fine-grained task of locating and explaining every trace. By focusing on human-perceived traces and grounding them in space and time, researchers can build detection tools that are both more accurate and more transparent. This grounding helps developers fix specific weaknesses in video generation, aids platforms in flagging problematic content with justifications, and provides a rigorous benchmark for evaluating how believable a video is from a human perspective. In short, multimodal reward modeling brings together what people see (visuals), hear (audio/text), and say (explanations) to produce a richer, more trustworthy guide for both evaluating and shaping AI-generated videos.\n\nPractical applications flow naturally from this setup. Content-moderation systems can use multimodal reward models to flag AI-made videos and show users where the fake traces lie, improving trust and safety. Researchers and engineers can use the ground-truth traces to diagnose and fix specific artifacts in generation pipelines, iterating toward more convincing (or honestly labeled) content depending on the goal. The same idea can be extended to other media types—audio deepfakes, manipulated images, or even combined generative tasks—where a model that can explain and localize its reasoning makes it easier to build responsible AI that aligns with human judgment."
  },
  "summary": "This paper introduces DeeptraceReward, a fine-grained benchmark of human-annotated deepfake traces in AI-generated videos and trains multimodal language models to mimic human judgments and localizations, providing a foundation for safer, more trustworthy video generation.",
  "paper_id": "2509.22646v1",
  "arxiv_url": "https://arxiv.org/abs/2509.22646v1",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.CL"
  ]
}
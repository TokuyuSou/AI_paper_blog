{
  "title": "Paper Explained: Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask - A Beginner's Guide",
  "subtitle": "Two Perspectives, One Conversation: A Fresh Look at Dialogue",
  "category": "Foundation Models",
  "authors": [
    "Nan Li",
    "Albert Gatt",
    "Massimo Poesio"
  ],
  "paper_url": "https://arxiv.org/abs/2511.03718v1",
  "read_time": "12 min read",
  "publish_date": "2025-11-06",
  "concept_explained": "Perspectivist Annotation Scheme",
  "content": {
    "background": "Imagine two teammates trying to plan a route using a map. They both think they’re on the same page, but one person labels a landmark with a nickname and the other uses a different name. Even when the conversation seems smooth, they might actually be referring to different things. In AI research on collaborative dialogue, people often asked whether participants eventually “get it,” but they didn’t look closely enough at cases where each person has a different interpretation. The tools and data available didn’t reliably show, for every reference expression, what the speaker meant and what the listener understood, making hidden mismatches hard to spot.\n\nThis is especially true when the people involved don’t share the same background or knowledge. In asymmetric settings, one person may know more or use a different vocabulary, yet the dialogue can still feel like agreement because the words sound familiar. Traditional studies tended to collapse those differences into a single back-and-forth outcome, which hides exactly where misinterpretations come from or how they creep back into later turns. Without a way to separate “what I mean” from “what you think I mean” for each reference, it’s tough to diagnose why grounding seems to work at a high level but fails in important details.\n\nUnderstanding grounding in this nuanced way matters for building AI that can truly collaborate with humans. If we want models that can participate in real conversations, they need to handle perspective-dependent meaning—to recognize when two people think they agree but are pointing at different things, and to anticipate where misunderstandings might arise. A new resource and annotation approach that marks speaker intent and addressee interpretation for each reference expression gives researchers a clear lens to study how understanding emerges, how it diverges, and how it gets repaired over time. This motivates better ways to train and evaluate AI systems on tasks that require genuine shared understanding, not just surface-level word matching.",
    "methodology": "Here’s the main idea in simple terms, with a clear step-by-step flavor you can use in class.\n\n- What’s new and why it matters (the big idea)\n  - The researchers ask: in a back-and-forth task where two people are trying to reach the same understanding, how do each person’s own perspective about what a reference refers to line up or diverge? They answer by creating a perspectivist annotation scheme that records, for every referring expression (like “the blue house” or “the big red barn”), two grounded interpretations: one from the speaker’s perspective and one from the addressee’s perspective. This lets them see not just whether words match, but whether each person is actually grounding those words to the same thing.\n  - Analogy: imagine two people looking at a map through different colored glass. Each person describes what they see, and the annotation scheme separately records what the speaker intends and what the listener understands, so you can trace where their views align or differ over time.\n\n- How they did it, conceptually (the methods in simple steps)\n  - Start with the MapTask dialogue: a classic setup where one person describes landmarks on a map for the other to reproduce, creating lots of opportunities for referential expressions.\n  - Define a clear labeling protocol: for every reference expression, mark both the speaker’s grounded interpretation and the addressee’s grounded interpretation, and track how these interpretations change as the conversation unfolds.\n  - Handle words, not just meanings: first unify lexical variants (synonyms or different phrasings for the same thing) so you’re focusing on whether people mean the same object, not whether they used the exact same words.\n  - Use a constrained, pipeline approach with a large language model (LLM): the team designs prompts and rules that nudge the LLM to produce the perspectivist annotations consistently, with built-in checks to estimate how reliable each annotation is.\n  - Build a dataset and analyze “understanding states”: assemble about 13,000 annotated reference expressions, each labeled with how the speaker and addressee grounded it, and whether the pair stayed aligned, diverged, or repaired over time.\n\n- What they found (the key insights)\n  - After removing lexical barriers, full-blown misunderstandings (both sides completely off) become rare. In other words, people often notice and adjust when the words themselves are the same.\n  - However, differences in multiplicity—the number of possible referents or how many objects each person thinks fit a description—systematically create divergences. Even if the same words are used, one person might be grounding to one object while the other grounds to a different but similar object.\n  - Importantly, surface grounding can look fine even when there’s referential misalignment underneath. In other words, it can seem like “we’re on the same page” when, in fact, the two perspectives are not truly aligned.\n  - These findings give a lens not only to study human dialogue more closely but also to evaluate how well (video/vision plus language) models and chat agents can handle perspective-dependent grounding in collaborative tasks.\n\n- Why this matters and what it enables\n  - Resource and tool: the approach yields a new dataset and a practical annotation pipeline focused on perspective-sensitive grounding, which other researchers can reuse to study misunderstandings or to train/evaluate models.\n  - Analytical lens: the perspectivist view helps researchers diagnose where grounding goes right or wrong, beyond just “do we agree on the word?”\n  - Implications for AI assistants: this work provides a framework for teaching or testing AI agents to recognize and manage different perspectives in collaborative tasks, which is crucial for real-world teamwork, negotiation, or instruction-giving scenarios.\n\nIn short, the paper adds a way to separately track what each person thinks a referring expression points to, over time, and then uses an LLM-guided workflow to label a large amount of data. This reveals that people can appear to agree while actually grounding to different things, and it gives researchers a concrete resource to study and improve how models handle perspective-based grounding in dialogue.",
    "results": "Here’s what the paper achieved in plain terms:\n\n- They created a new way to study grounding (how people link words to real things) that keeps track of each person’s point of view. In their MapTask data, they labeled not just what a speaker meant, but also what the listener understood from the speaker’s words. In other words, they separately record the speaker’s grounding and the addressee’s grounding for every reference expression. This “perspectivist” approach helps you see where people think they agree even when they don’t.\n\n- They built a scalable annotation pipeline using a constrained large language model (LLM) to label about 13,000 reference phrases according to this perspectivist scheme, with reliability checks. This is important because it shows you can mix AI help with careful human oversight to produce a large, trustworthy dataset about grounding and misunderstandings, not just a handful of tiny examples.\n\n- They then analyzed how understanding changes over time during dialogue. A key finding is that once you unify different ways people refer to the same thing (lexical variants), full-blown misunderstandings become rare. But when people disagree about how many referents or which properties count (multiplicity discrepancies), divergences naturally appear. In short: it can look like everyone is on the same page, even when their underlying references are misaligned.\n\nWhat makes this work significant and its practical impact\n\n- New lens for studying dialogue: Before, researchers often treated “understanding” as a single thing. This work shows that two people can seem to agree while actually grounding differently. The perspectivist scheme is a practical tool for diagnosing those subtle misalignments in real-time.\n\n- A valuable resource for AI evaluation: The 13k labeled expressions, plus reliability estimates, give researchers a concrete dataset to test whether AI systems (including vision-language models and chat models) can handle perspective-dependent grounding in collaborative tasks. This helps push AI from simply following words to tracking how different people mentally link words to things.\n\n- Real-world impact for better collaborative AI: By highlighting when misunderstandings actually arise (mostly due to perspective differences rather than vocabulary alone), this approach can guide the design of smarter assistants, tutoring systems, or robots that collaborate with humans. Such systems could detect when a user’s perspective diverges from theirs and prompt repairs, leading to smoother teamwork in areas like education, design, and remote collaboration.",
    "significance": "This paper tackles a core problem in collaborative AI: people (or agents) can think they share the same understanding even when they’re actually referring to different things. The authors introduce a perspectivist annotation scheme that separately records what the speaker and the addressee grounding think a reference expression refers to, and they apply this to the MapTask dialogue. By combining this with a pipeline that uses large language models to annotate thousands of references, they can trace how understanding arises, diverges, and is repaired over time. The result is both a new resource (a richly annotated dataset) and a new way to analyze grounding in dialogue, which matters today because real-world AI systems increasingly work with imperfect, evolving shared context.\n\nThe paper influenced later work by formalizing how to separate and track perspective-dependent grounding in multi-turn dialogue. This perspective-aware lens helps researchers evaluate and improve how AI systems maintain common ground, disambiguate references, and repair misunderstandings during collaboration. As a consequence, it fed into methods and benchmarks for assessing grounded language understanding in interactive settings, and it encouraged the development of evaluation tools for (V)LLMs in tasks that require perspective-sensitive reasoning, referential consistency, and cooperative problem-solving. Applications that benefited include collaborative editing tools, negotiation or planning assistants, and human–robot interaction systems where keeping the same reference and goal in view is crucial.\n\nConnecting to modern AI systems like ChatGPT, GPT-4, or Claude, the paper’s ideas remain highly relevant. Today, these models often produce coherent responses but can still slip on perspective and referential grounding in multi-turn, asymmetric settings (where one party has more or different information). The perspectivist approach provides a principled way to diagnose and quantify such grounding mismatches, and it can inform prompts or system messages that encourage models to explicitly align with the user’s references, or to reveal and repair misalignments. In the long run, this line of work helps build safer, more cooperative AI that can reliably share a common ground with humans and other agents, which is essential for any AI that participates in collaborative tasks or long conversations."
  },
  "concept_explanation": {
    "title": "Understanding Perspectivist Annotation Scheme: The Heart of Grounded Misunderstandings in Asymmetric Dialogue",
    "content": "Imagine two teammates giving directions to find a hidden treasure on a map. They both want to get to the same place, but they might rely on different clues or label things differently. One person might refer to “the big red square” while the other uses “the large crimson block.” Even if they’re both pointing to the same region, it can look like they’re talking about different spots. The Perspectivist Annotation Scheme is a careful way of recording exactly how each reference expression is grounded for both sides: what the speaker intends (speaker-grounded) and what the listener understands (addressee-grounded). This lets researchers see not just what was said, but how understanding can match, differ, or break down over time in a real dialogue.\n\nHow does it work, step by step? First, you find a reference expression in the dialogue—a phrase that points to something on the map, like “the river” or “the second bridge.” Next, you record two perspectives for that expression. The speaker-grounded interpretation answers: which object or feature does the speaker think of, and what attributes or relations do they rely on (color, shape, location, order)? The addressee-grounded interpretation answers: if the listener tries to locate the referent, what would they think it is, given their own knowledge and viewpoint? The process also involves unifying lexical variants so that different words that refer to the same thing aren’t mistaken for different referents (for example, “river” vs. “stream”). Then an annotation pipeline—driven by constraints from the Perspectivist Scheme and supported by a language model (LLM)—produces labels for many expressions at scale, with reliability estimates from multiple annotators. Finally, researchers analyze how the pairings of speaker-grounded and addressee-grounded interpretations evolve over time, revealing moments of alignment, divergence, or repair.\n\nA concrete example helps. Suppose in a MapTask dialogue the speaker says, “Take the second bridge east of the big red square.” The speaker-grounded reading would pin this reference to a specific bridge, with reasoning like: “second bridge along the eastward path from the starting point, near the big red square.” But the addressee-grounded reading might differ: perhaps the listener counts bridges differently (maybe they count from a different starting point or disagree on what counts as “east of”). If there are several blue triangles in the scene, the speaker might refer to “the blue triangle,” while the listener grounds it to the one closest to the river—leading to a misalignment that could cause a detour. The Perspectivist Scheme records both viewpoints for every reference, so researchers can see how small wording differences (like “second” vs. “another”) or different groundings (color/shape vs. location) lead to understanding gaps, and how those gaps get repaired during the conversation.\n\nWhy is this important? It gives researchers a precise lens to study grounded misunderstandings in collaborative dialogue, especially when participants don’t share the same perspective. For AI and language models, it provides a valuable testbed to see whether models can track perspective-dependent grounding: do they understand that the same phrase might refer to different things for different people, and can they help the speakers repair misalignments? The MapTask corpus, annotated with speaker- and addressee-grounded interpretations (about 13,000 reference expressions in total), becomes a rich resource for evaluating and training models to reason about perspective in real conversations. This approach helps designers build better dialogue systems, robots, or educational tools that can detect when grounding diverges and offer clarifications, keeping teamwork smooth even when people have incomplete or asymmetric information.\n\nIn short, the Perspectivist Annotation Scheme adds a dual-view map to every reference in a dialogue: how the speaker grounds it and how the addressee grounds it. This makes it possible to trace how understanding forms, drifts, and is repaired over time, and to use that insight to build AI that better handles perspective and grounding in collaborative tasks. Practical applications range from improving human-robot collaboration and virtual assistants to creating robust datasets for evaluating how well models model perspective-dependent grounding. It’s a step toward AI that can reason about \"whose view counts\" in a conversation, not just what is literally said."
  },
  "summary": "This paper introduces a perspectivist annotation scheme for the MapTask dialogue that separately records speaker and listener grounding for every reference, uses a scheme-constrained LLM pipeline to annotate thousands of expressions, and shows how understanding can diverge even when participants think they agree, providing a new resource and method to study grounded misunderstandings and to evaluate AI models on perspective-based grounding.",
  "paper_id": "2511.03718v1",
  "arxiv_url": "https://arxiv.org/abs/2511.03718v1",
  "categories": [
    "cs.CL",
    "cs.AI"
  ]
}
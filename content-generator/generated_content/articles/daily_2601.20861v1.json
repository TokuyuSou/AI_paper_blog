{
  "title": "Paper Explained: Evolutionary Strategies lead to Catastrophic Forgetting in LLMs - A Beginner's Guide",
  "subtitle": "Continuous learning forgets what it learned",
  "category": "Foundation Models",
  "authors": [
    "Immanuel Abdi",
    "Akshat Gupta",
    "Micah Mok",
    "Alexander Lu",
    "Nicholas Lee",
    "Gopala Anumanchipalli"
  ],
  "paper_url": "https://arxiv.org/abs/2601.20861v1",
  "read_time": "10 min read",
  "publish_date": "2026-01-29",
  "concept_explained": "Catastrophic Forgetting",
  "content": {
    "background": "- Before this work, a big dream in AI was to have models that keep learning after they’re deployed, without needing to rewind and retrain from scratch. The usual way to teach these models uses gradients and a lot of memory and compute, which isn’t practical for online, continual learning. A gradient-free approach called Evolutionary Strategies (ES) looked promising as a lightweight alternative, but nobody had clearly asked whether ES could actually learn new things without wrecking what the model already knew. The motivation was to find out if ES is a viable path for continual learning in large language models.\n\n- The researchers asked how ES behaves as you keep updating the model more and more. Do the updated models keep their old abilities, or do they forget them? They compared ES to traditional gradient-based learning on math and reasoning tasks, and found that ES can reach competitive performance with similar compute. But the big catch is that the gains come with substantial forgetting: as ES updates pile up, old skills degrade, making ES much less suitable for online learning than one would hope. In other words, ES tends to make broad, disruptive changes to the model rather than small, careful tweaks.\n\n- This matters because, in the real world, we want AI systems that can accumulate knowledge over time without losing what they already learned. If a gradient-free method like ES tends to erase prior abilities, its usefulness for continual learning is limited unless we can fix that forgetting. The paper’s motivation is to highlight this forgetting pitfall and to spur future work toward reducing forgetting in gradient-free approaches or developing better continual-learning strategies.",
    "methodology": "Here’s the big idea in plain terms. The researchers are asking: can a gradient-free learning method called Evolutionary Strategies (ES) train large language models (LLMs) over time, without storing and updating gradients like traditional backpropagation does? They run a careful study to see not only how well ES learns new tasks (like math and reasoning) but also how it forgets old abilities as you keep updating the model. They compare ES to a strong gradient-based method (GRPO) and focus on how performance changes when you perform more updates. The punchline is nuanced: ES can get close to GRPO on some tasks with similar compute, but it also tends to wipe out earlier skills as you keep updating, which is bad for continual learning.\n\nHow they did it, conceptually, can be broken into simple steps:\n- Step 1: Set up the experiment with a large language model and a mix of tasks (especially math and reasoning) that the model should learn.\n- Step 2: Train with Evolutionary Strategies, making a sequence of update rounds, and keep the total compute budget comparable to the gradient-based method.\n- Step 3: After each round (or after several rounds), evaluate how well the model does on the new tasks and also on tasks it already learned, to see if earlier abilities fade away.\n- Step 4: Do a parallel analysis with a gradient-based method to contrast what the parameter updates look like and how they affect knowledge retention.\n- Step 5: Look under the hood conceptually: what do the ES updates actually do to the model’s parameters compared with gradient-based updates? Do ES updates touch lots of parameters and with big changes, or are they more targeted and smaller?\n\nWhy this leads to forgetting, in plain language, is the key takeaway. ES works by probing many slight changes to lots of parameters and then selecting updates that improve performance. In practice, this tends to produce updates that are less sparse (affecting many parameters) and larger in total change than the gradient-based updates. Think of it like turning a dial-spread knob that suddenly makes broad, sweeping changes to the model’s behavior, rather than carefully nudging just a few important gears. The result is that while ES can reach competitive performance on certain tasks, those broad, sweeping updates can overwrite or disrupt previously learned skills, causing “catastrophic forgetting” when you try to accumulate knowledge over time. The study uses this contrast to explain why ES may be more prone to forgetting than gradient-based methods, even though it’s attractive for being gradient-free.\n\nFor future work, the paper points toward practical ways to mitigate this forgetting if we want ES to be viable for continual learning:\n- Make updates sparser or more targeted, so that only a small, careful subset of parameters changes at a time.\n- Add mechanisms to protect important parameters, such as regularizers that discourage big changes to features that matter for previously learned tasks.\n- Use memory or rehearsal techniques that periodically remind the model of earlier tasks, helping retention across updates.\n- Explore hybrid approaches that combine the strengths of ES (gradient-free exploration) with some gradient-based signals to guide updates more selectively.\n\nIn short, the study asks a clear question about continual learning with ES, shows both its potential and its limits, and identifies a core reason—how widely and aggressively ES updates alter the model—that helps guide future efforts to make gradient-free learning more retirement-safe for long-lived AI systems.",
    "results": "This paper studies Evolutionary Strategies (ES) as a gradient-free way to train large language models (LLMs). ES is like trying lots of random edits to the model and keeping the edits that help, instead of calculating precise gradients. The authors compare ES to a traditional gradient-based method (GRPO) and find that on math and reasoning tasks, ES can reach similar performance with comparable compute. That means ES can be surprisingly competitive for certain skills, which is interesting because it uses a very different learning signal than usual.\n\nBut the big takeaway is about continual learning—getting a model to keep its old abilities while it learns new ones online. The researchers show that, as you push ES to train longer or more steps, the model forgets earlier skills badly. In other words, ES is great at reaching good performance quickly on some tasks, but it struggles to remember what it learned before when it keeps updating. The reason, they find, is that ES makes updates that are larger and less selective than those from the gradient-based method, which go through the model more gently and preserve prior knowledge better. Those sweeping, frequent changes tend to wipe out previously learned abilities.\n\nThe practical impact is clear: ES is not a drop-in solution for online continual learning in LLMs. It can match gradient-based methods on some tasks, but its tendency to catastrophically forget limits its usefulness for systems that need to learn continuously after deployment. This work highlights a fundamental challenge for gradient-free approaches and points to future research directions, such as developing strategies to make ES updates safer for memory, or hybrid approaches that combine ES with mechanisms to protect or replay old knowledge. The study helps clarify where ES can be helpful and what must be solved before it can reliably support ongoing learning in real-world AI systems.",
    "significance": "This paper matters today because there’s a growing push to have AI systems that can learn after they’re deployed—think of an assistant that improves over time as it chats with people. Evolutionary Strategies (ES) looked like a promising, memory-friendly alternative to traditional gradient-based training, which needs huge compute and memory. The authors show a careful truth: ES can reach near-state performance on some math and reasoning tasks with similar compute, but when you let ES run for more updates, the model starts to forget what it learned before. In simple terms, ES makes big, sweeping changes to the model’s internal knobs, and that ends up erasing older skills. This is a key reminder that “learning more” in one shot is not the same as building a system that can learn continually without losing its earlier abilities.\n\nIn the long run, this work helped steer the AI research community toward thinking harder about forgetting in gradient-free approaches, not just in gradient-based ones. It sparked follow-up work that asks how to make ES and other gradient-free methods safer for continual learning—things like adding memory replay (replaying old examples), using modular or sparse updates, or combining ES with gradient signals so updates don’t wipe out prior knowledge. Researchers also began to develop evaluation benchmarks that explicitly measure forgetting for online or lifelong learning scenarios. The insight that ES updates are dense and can have large changes in the model’s parameters helped explain why some online adaptation methods fail and why future designs must carefully manage update magnitude and selectivity.\n\nThis matters for real-world systems people use every day, including ChatGPT-like chatbots, personal assistants, and customer-service bots that need to adapt to new information or user preferences without losing core skills. The paper’s lessons underpin modern approaches to continual learning in large models, such as using adapters or modular components (instead of sweeping full-model updates), memory-based or retrieval-augmented techniques, and careful update scheduling. In short, it’s a foundational caution and a blueprint: if we want AI like ChatGPT to improve after deployment, we must design learning processes that add knowledge while preserving what the model already does well."
  },
  "concept_explanation": {
    "title": "Understanding Catastrophic Forgetting: The Heart of Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
    "content": "Think of learning like teaching a student to do several tasks on a single talent, like playing piano pieces. If you keep adding new pieces and you practice by making big, sweeping changes to how the fingers move across the keyboard, you might start playing the new pieces well but forget how to play the older ones. In AI, “catastrophic forgetting” is that same idea: when a model is trained on new tasks, it can lose the ability it had for earlier tasks.\n\nHere’s how Evolutionary Strategies (ES) work in simple terms, step by step. You have a big network with lots of numbers (weights) that tell it how to think. ES doesn’t use the usual backpropagation to compute tiny adjustments. Instead, it creates many slightly different copies of the model by nudging the weights with random noise. Each copy is tested on a set of tasks (like math problems or reasoning puzzles), and the results tell you which nudges helped the overall performance. You then move the original model a bit in the direction that looked best, based on those tests. Because ES evaluates whole versions of the model, the final update can touch many weights all at once and with relatively large changes, rather than tweaking a small subset of weights.\n\nThe paper you cite finds a key trade-off. ES can get performance close to a gradient-based method (on certain math and reasoning tasks) if you give it a similar amount of compute. But when you train longer—trying to improve more and more—it starts forgetting what it had learned before. In other words, the model gets better on new tasks but worse on old ones as training continues. The authors show this isn’t just a tiny drift: the ES updates are (a) far less sparse, meaning more weights change, and (b) have much larger L2 norms, meaning bigger overall shifts in weight values. That combination makes it more likely that the model overwrites or disrupts knowledge it had acquired earlier, causing the forgetting you see.\n\nWhy does this matter in practice? Real-world AI systems often need to learn continuously after deployment—think a chat assistant that updates with new product info, or a knowledge base that grows over time. If the learning method causes big, widespread changes every time it learns something new, it can lose what it already knows, making the system unreliable. This paper highlights that even gradient-free approaches like ES aren’t immune to forgetting and calls attention to the need for strategies to protect old knowledge. Practical remedies discussed in the broader literature include replaying old data, regularizing updates to limit how much weights can drift, freezing parts of the network, or using layouts that keep different kinds of knowledge separate (modular design). A takeaway for students is that continual learning isn’t just about teaching new things; it’s also about remembering old things, and the way you update a model’s parameters plays a big role in that balance."
  },
  "summary": "This paper studies Evolutionary Strategies for training LLMs and shows that, although ES can achieve comparable math and reasoning performance with similar compute, it causes catastrophic forgetting of earlier abilities in continual learning because its updates are larger and less sparse than those of gradient-based methods.",
  "paper_id": "2601.20861v1",
  "arxiv_url": "https://arxiv.org/abs/2601.20861v1",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ]
}